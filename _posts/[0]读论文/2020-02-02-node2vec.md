---
layout: post
title: 【读论文2016】node2vec
categories:
tags: 0-读论文
keywords:
description:
order: 6
---

node2vec: Scalable Feature Learning for Networks

## 本文贡献
1. 提出了 node2vec 模型
2. 展示了 node2vec 模型，符合以往的网络科学（network science）中建立起来的原则
3. 拓展了 node2vec 算法，以及其他基于邻居的算法，来做基于边的预测任务
4. 在真实数据上，建立 **标签分类** （multi-label classification） 和 **连接预测** ( link prediction) 这两种任务，目的是经验性地评估 node2vec （为啥是经验性地评估呢？个人觉得是因为这用两个任务评估算法有一定的说服力，但并不充分，但也没有更好的评估方法了）

## 前人成果
我们想要寻找一种无须任何人工工程（hand-engineered）的方法

无监督特征抽取，往往需要各种矩阵，例如拉普拉斯矩阵和近邻矩阵（Laplacian and the adjacency matrice），一代数视角看来，这些方法是一些降维技术，例如PCA和IsoMap，他们的缺点来自两个：计算量、统计表现。
- 特征分解就很耗费计算量。所以这些方法很难用到大规模网络上
- 对于各种网络特征，并不鲁棒。例如谱聚类（spectral clustering）的前提假设就很强（也就是很严，不过好多中文也喜欢用强这个词）。

然后论文介绍了NLP中的 word2vec 方法，然后说可以把节点序列类比为文字序列。这一块不多说。  

但是，有很多种采样策略，论文的研究展示，没有哪个采样策略显著胜出。  
node2vec 就克服了这一缺陷，靠的是设计了一个灵活的、不依赖特定采样策略的目标，以及提供了可以调整的参数。

## node2vec框架

摘录一个原文上的公式：  
$\max\limits_f \sum\limits_{u\in V} Pr(N_S(u)\mid f(u))$  

其中，
- V是节点，图 $G=(V,E)$ 的节点。  
- $N_S$ 是一种采样方法，$N_S(u)$ 是用这种采样方法得到的邻居。（后面好几段都在强调这个采样方法是创新点）

NLP 中的 skip-gram，因为语言是线性的，所以直接去截取就行了。但图上的不能这样做。


### 传统的采样方法
有两个极端：
- Breadth-first Sampling (BFS)
- Depth-first Sampling (DFS)

这个不多说，看下面这个图

![node2vec1](/pictures_for_blog/papers/recommended_system/node2vec1.gif)


通常说，图上的点的预测问题，往往是两种相似性的折中：同质性和结构等价性。  
（原文：prediction tasks on nodes in networks often shuttle between two kinds of similarities: **homophily** and **structural equivalence**）
- **homophily**:如果两个节点连接紧密，那么就应当归为一类。例如图中的s1和u就在homophily概念上算同一社区。
- **structural equivalence**：有相似的结构上的角色（e similar structural roles in networks），例如u和s6在结构上就很相似。现实中，两个节点可以相距很远，但有相同的结构。


一般来说，
- BFS取样更多的提取 **homophily** 特征。直观理解：**structural equivalence** 更加依赖相邻的邻居，BFS会更多提取相邻邻居的信息。再者，BFS采样会把相邻邻居采样后的数量增多。
- DFS采样更多地提取更宏观的视角，


## node2vec
### random walk
以概率随机游走，deep walk 那篇论文笔记里也写了，不多说。

### search bias
